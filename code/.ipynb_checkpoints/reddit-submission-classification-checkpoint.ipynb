{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Natural Language Processing to Predict Subreddit Submissions\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Reddit is one of the most popular sites on the internet with over 11 million daily active users. The variety and breadth of subreddits allows ample oppurtinites to explore data collection and machine learning techniques. Here we will collect the submissions from three subreddits and build a variety of classifier models in an effort to predict from which subreddit a post is from.\n",
    "\n",
    "### Subreddits\n",
    "\n",
    "In order to build a rigorous model subreddits with overlapping themes were chosen.\n",
    "- r/whiskey\n",
    "- r/scotch\n",
    "- r/bourbon\n",
    "\n",
    "1. Not all whiskies are Bourbon.\n",
    "\n",
    "2. Not all whiskies are Scotch.\n",
    "\n",
    "3. Both Scotch and Bourbon are whiskies.\n",
    "\n",
    "The following models will be compared to see if any can beat a baseline calculation of prediction.\n",
    "- Random Forest Classifier\n",
    "- Multinomial Naive Bayes\n",
    "- AdaBoost Classifier\n",
    "- Gradient BoostingClassifier\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "### Contents:\n",
    "- [Imports and Options](#Imports-and-Options)\n",
    "- [Data](#Data)\n",
    "    * [Data Collection](#Data-Collection)\n",
    "    * [Data Analysis](#Data-Analysis)\n",
    "    * [Data Cleaning](#Data-Cleaning)\n",
    "    * [Data Dictionary](#Data-Dictionary)\n",
    "- [Modeling](#Modeling)\n",
    "    * [Model Preperation](#Model-Preperation)\n",
    "    * [Model Proceessing](#Model-Processing)\n",
    "- [Results](#Results)\n",
    "    * [Conclusions](#Conclusions) \n",
    "    * [Recommendations and Next Steps](#Recommendations-and-Next-Steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Options\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "\n",
    "Data was collected from reddit.com using the pushshift api.\n",
    "\n",
    "https://github.com/pushshift/api\n",
    "\n",
    "Data was collected using the reddit-subbmissions-pull.py file in the [code](./code) folder of this repository. For the purposes of this exercise five years of data was collected from 2014 to 2019.\n",
    "\n",
    "Each subreddit's data is stored as a separate csv in the datasets folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all three csv's as dataframes\n",
    "df_whiskey = pd.read_csv('./datasets/whiskey_submissions.csv')\n",
    "df_scotch  = pd.read_csv('./datasets/scotch_submissions.csv')\n",
    "df_bourbon = pd.read_csv('./datasets/bourbon_submissions.csv')\n",
    "\n",
    "print(df_whiskey.shape)\n",
    "print(df_scotch.shape)\n",
    "print(df_bourbon.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19380, 101)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bourbon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging all dataframes\n",
    "df = pd.concat([df_whiskey, df_scotch, df_bourbon], ignore_index=True, sort=True)\n",
    "df.reset_index().head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "The pushshift api returns a large variety of meta data from each submission. For the purposes of comparing natural language processing  between classification models the following features were extracted.\n",
    "\n",
    "- title\n",
    "- selftext\n",
    "- score\n",
    "- num_comments\n",
    "- subreddit\n",
    "\n",
    "Even though score and number of comments are numerical sets of data they are an intrensical aspect of how a subreddit may operate. How active a subreddit is via it's karma score and general discussion could be used as a way to infer the the origin of a post.\n",
    "\n",
    "#### Reasons for lack of stemming and lemmatization.\n",
    "Whiskeys are primarily known by their type and distillery. Given the wide range of names for Scottish distilleries there was concern that applying a stemming or lemmatization transformation could have a negative effect on model performance.\n",
    "\n",
    "Examples:\n",
    " - Glentauchers, Mulben\n",
    " - Laphroaig, Port Ellena\n",
    " - Benrinnes, Banffshire\n",
    " \n",
    "Additionally both vectorizers will have a the n-gram range set to (1,2) in the hyperparameter search to account for distilleries with multiples words for their names or locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['title', 'selftext', 'score', 'num_comments', 'subreddit']]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since reddit doesn't require a poster describe their post there are many nulls for selftext\n",
    "# We will replace all nulls with a placeholder string that will be passed as a stopword\n",
    "df.fillna('stopwordplaceholder', inplace=True)\n",
    "\n",
    "# Creating a list to pass into stop_words parameter for Transformers\n",
    "stopwordplaceholder = ['stopwordplaceholder']\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary\n",
    "\n",
    "| Variable           | Variable Name | Data Type                  | Description                                                                                                                              |\n",
    "|--------------------|---------------|----------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Title              | title         | str                        | Tite of the post. This is what will appear on the subreddit and is mandatory                                                             |\n",
    "| Post Description   | selftext      | str                        | A user can provide a description for the post that elaborates on the point or is to begin a discussion. This is not required for a post. |\n",
    "| Number of Comments | num_comments  | int                        | Number of comments in the post thread. Submission does not count as comment.                                                             |\n",
    "| Score              | score         | int (positive or negative) | Reddit users and upvote or downvote a post based on its relevance or general enjoyment.                                                  |\n",
    "| Subreddit          | subreddit     | str                        | The subreddit the submission is from.                                                                                                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Preperation\n",
    "\n",
    "Since models may require different transformation to the days we will train/test/split first.\n",
    "\n",
    "Due to proccessing limitations inital analysis will be performed only on the 'title' aspect of the submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test/Split\n",
    "X = df['title']\n",
    "y = df['subreddit']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45782,)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15261,)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Processing\n",
    "\n",
    "Each of the models listed will be run with similar hyperparameters where applicable.\n",
    "\n",
    "Additionally each model will be transformed by both a CountVectorizer and TFIDFVectorizer.\n",
    "\n",
    "\n",
    "- [Random Forest Classifier](#Random-Forest-Classifier)\n",
    "- [Multinomial Naive Bayes](#Multinomial-Naive-Bayes)\n",
    "- [AdaBoost Classifier](#AdaBoost-Classifier)\n",
    "- [Gradient Boost Classifier](#Gradient-Boost-Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a CrossVectorizer Pipe Parameter Dictionary\n",
    "cvec_pipe_params = {\n",
    "    'cvec__max_features'  : [2000, 3000, 5000],\n",
    "    'cvec__min_df'        : [2, 3],\n",
    "    'cvec__max_df'        : [.9],\n",
    "    'cvec__ngram_range'   : [(1,1), (1,2)],\n",
    "    'cvec__stop_words'    : [stopwordplaceholder],\n",
    "    'cvec__strip_accents' : ['unicode']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a TFIDFVectorizer Pipe Parameter Dictionary\n",
    "tfidf_pipe_params = {\n",
    "    'tfidf__max_features' : [2000, 3000, 5000],\n",
    "    'tfidf__min_df'       : [2, 3],\n",
    "    'tfidf__max_df'       : [.9],\n",
    "    'tfidf__ngram_range'  : [(1,1), (1,2)],\n",
    "    'tfidf__stop_words'   : [stopwordplaceholder],\n",
    "    'tfidf__strip_accents': ['unicode']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating RandomForest Parameter Dictionary\n",
    "rf_pipe_params = {\n",
    "    'rf__n_estimators' : [50, 100, 150],\n",
    "    'rf__max_depth'    : [None, 5],\n",
    "    'rf__n_jobs'       : [6]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Pipline\n",
    "rf_cvec_pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Creating GridSearch\n",
    "\n",
    "# Temporary parameters dict\n",
    "temp_params_dict = {**cvec_pipe_params, **rf_pipe_params}\n",
    "\n",
    "gs_rf_cvec = GridSearchCV(rf_cvec_pipe,\n",
    "                 temp_params_dict,\n",
    "                 cv=5)\n",
    "\n",
    "# Fitting Gridsearch Data\n",
    "# Setting timer\n",
    "t0 = time.time()\n",
    "\n",
    "gs_rf_cvec.fit(X_train, y_train)\n",
    "\n",
    "rf_cvec_time = time.time() - t0\n",
    "\n",
    "# Calling Scores and Best Parameters\n",
    "best_rf_cvec_train_score =  gs_rf_cvec.score(X_train, y_train)\n",
    "best_rf_cvec_test_score  =  gs_rf_cvec.score(X_test, y_test)\n",
    "best_rf_cvec_params      =  gs_rf_cvec.best_params_\n",
    "print(rf_cvec_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDFVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Pipline\n",
    "rf_tfidf_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Creating GridSearch\n",
    "\n",
    "# Temporary parameters dict\n",
    "temp_params_dict = {**tfidf_pipe_params, **rf_pipe_params}\n",
    "\n",
    "gs_rf_tfidf = GridSearchCV(rf_tfidf_pipe,\n",
    "                 temp_params_dict,\n",
    "                 cv=5)\n",
    "\n",
    "# Fitting Gridsearch Data\n",
    "# Setting timer\n",
    "t0 = time.time()\n",
    "\n",
    "gs_rf_tfidf.fit(X_train, y_train)\n",
    "\n",
    "rf_tfidf_time = time.time() - t0\n",
    "\n",
    "# Calling Scores and Best Parameters\n",
    "best_rf_tfidf_train_score =  gs_rf_tfidf.score(X_train, y_train)\n",
    "best_rf_tfidf_test_score  =  gs_rf_tfidf.score(X_test, y_test)\n",
    "best_rf_tfidf_params      =  gs_rf_tfidf.best_params_\n",
    "print(rf_tfidf_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating MultinomialNB Pipeline Params\n",
    "mnb_pipe_params = {\n",
    "    'mnb__alpha' : [.01, .1, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Pipline\n",
    "mnb_cvec_pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Creating GridSearch\n",
    "\n",
    "# Temporary parameters dict\n",
    "temp_params_dict = {**cvec_pipe_params, **mnb_pipe_params}\n",
    "\n",
    "gs_mnb_cvec = GridSearchCV(mnb_cvec_pipe,\n",
    "                 temp_params_dict,\n",
    "                 cv=5)\n",
    "\n",
    "# Fitting Gridsearch Data\n",
    "# Setting timer\n",
    "t0 = time.time()\n",
    "\n",
    "gs_mnb_cvec.fit(X_train, y_train)\n",
    "\n",
    "mnb_cvec_time = time.time() - t0\n",
    "\n",
    "# Calling Scores and Best Parameters\n",
    "best_mnb_cvec_train_score =  gs_mnb_cvec.score(X_train, y_train)\n",
    "best_mnb_cvec_test_score  =  gs_mnb_cvec.score(X_test, y_test)\n",
    "best_mnb_cvec_params      =  gs_mnb_cvec.best_params_\n",
    "print(mnb_cvec_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDFVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Pipline\n",
    "mnb_tfidf_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Creating GridSearch\n",
    "\n",
    "# Temporary parameters dict\n",
    "temp_params_dict = {**tfidf_pipe_params, **mnb_pipe_params}\n",
    "\n",
    "gs_mnb_tfidf = GridSearchCV(mnb_tfidf_pipe,\n",
    "                 temp_params_dict,\n",
    "                 cv=5)\n",
    "\n",
    "# Setting timer\n",
    "t0 = time.time()\n",
    "\n",
    "# Fitting Gridsearch Data\n",
    "gs_mnb_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# Storing timer\n",
    "mnb_tfidf_time = time.time() - t0\n",
    "\n",
    "# Calling Scores and Best Parameters\n",
    "best_mnb_tfidf_train_score =  gs_mnb_tfidf.score(X_train, y_train)\n",
    "best_mnb_tfidf_test_score  =  gs_mnb_tfidf.score(X_test, y_test)\n",
    "best_mnb_tfidf_params      =  gs_mnb_tfidf.best_params_\n",
    "print(mnb_tfidf_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting AdaBoostClassifier Pipeline Parameters\n",
    "ada_pipe_params = {\n",
    "    'ada__n_estimators'              : [50, 100, 150],\n",
    "    'ada__base_estimator__max_depth' : [1, 2],\n",
    "    'ada__learning_rate'             : [.9, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Pipeline\n",
    "ada_cvec_pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('ada', AdaBoostClassifier(base_estimator=DecisionTreeClassifier()))\n",
    "])\n",
    "\n",
    "# Creating Gridsearch paramaters dictionary\n",
    "temp_params_dict = {**cvec_pipe_params, **ada_pipe_params}\n",
    "\n",
    "# Creating GridsearchCV\n",
    "gs_ada_cvec = GridSearchCV(ada_cvec_pipe,\n",
    "                           temp_params_dict,\n",
    "                           cv=5)\n",
    "\n",
    "# Setting Timer\n",
    "t0 = time.time()\n",
    "\n",
    "# Fitting Gridsearch\n",
    "gs_ada_cvec.fit(X_train, y_train)\n",
    "\n",
    "# Storimg Timer\n",
    "ada_cvec_time = time.time() - t0\n",
    "\n",
    "# Calling scores and best parameters\n",
    "best_ada_cvec_train_score = gs_ada_cvec.score(X_train, y_train)\n",
    "best_ada_cvec_test_score  = gs_ada_cvec.score(X_test, y_test)\n",
    "best_ada_cvec_params      = gs_ada_cvec.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDFVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Pipeline\n",
    "ada_tfidf_pipe = Pipeline([\n",
    "    ('tfidf', CountVectorizer()),\n",
    "    ('ada', AdaBoostClassifier(base_estimator=DecisionTreeClassifier()))\n",
    "])\n",
    "\n",
    "# Creating Gridsearch paramaters dictionary\n",
    "temp_params_dict = {**tfidf_pipe_params, **ada_pipe_params}\n",
    "\n",
    "# Creating GridsearchCV\n",
    "gs_ada_tfidf = GridSearchCV(ada_tfidf_pipe,\n",
    "                           temp_params_dict,\n",
    "                           cv=5)\n",
    "\n",
    "# Setting Timer\n",
    "t0 = time.time()\n",
    "\n",
    "# Fitting Gridsearch\n",
    "gs_ada_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# Storimg Timer\n",
    "ada_tfidf_time = time.time() - t0\n",
    "\n",
    "# Calling scores and best parameters\n",
    "best_ada_tfidf_train_score = gs_ada_tfidf.score(X_train, y_train)\n",
    "best_ada_tfidf_test_score  = gs_ada_tfidf.score(X_test, y_test)\n",
    "best_ada_tfidf_params      = gs_ada_tfidf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting GradienBoostClassifier pipeline parameters\n",
    "gb_pipe_params = {\n",
    "    'gb__n_estimators'  : [50, 100, 150],\n",
    "    'gb__max_depth'      : [1, 2],\n",
    "    'gb__learning_rate' : [.9, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating pipeline\n",
    "gb_cvec_pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "# Creating gridsearch parameters dictionary\n",
    "temp_params_dict = {**cvec_pipe_params, **gb_pipe_params}\n",
    "\n",
    "# Creating GridsearchCV\n",
    "gs_gb_cvec = GridSearchCV(gb_cvec_pipe,\n",
    "                          temp_params_dict,\n",
    "                          cv=5)\n",
    "\n",
    "# Setting Timer\n",
    "t0 = time.time()\n",
    "\n",
    "# Fitting Gridsearch\n",
    "gs_gb_cvec.fit(X_train, y_train)\n",
    "\n",
    "# Storing Timer\n",
    "gb_cvec_time = time.time() - t0\n",
    "\n",
    "# Calling scores and best parameters\n",
    "best_gb_cvec_train_score = gs_gb_cvec.score(X_train, y_train)\n",
    "best_gb_cvec_test_score  = gs_gb_cvec.score(X_test, y_test)\n",
    "best_gb_cvec_params      = gs_gb_cvec.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDFVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating pipeline\n",
    "gb_tfidf_pipe = Pipeline([\n",
    "    ('tfidf', CountVectorizer()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "# Creating gridsearch parameters dictionary\n",
    "temp_params_dict = {**tfidf_pipe_params, **gb_pipe_params}\n",
    "\n",
    "# Creating GridsearchCV\n",
    "gs_gb_tfidf = GridSearchCV(gb_tfidf_pipe,\n",
    "                          temp_params_dict,\n",
    "                          cv=5)\n",
    "\n",
    "# Setting Timer\n",
    "t0 = time.time()\n",
    "\n",
    "# Fitting Gridsearch\n",
    "gs_gb_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# Storing Timer\n",
    "gb_tfidf_time = int(round(time.time() - t0))\n",
    "\n",
    "# Calling scores and best parameters\n",
    "best_gb_tfidf_train_score = gs_gb_tfidf.score(X_train, y_train)\n",
    "best_gb_tfidf_test_score  = gs_gb_tfidf.score(X_test, y_test)\n",
    "best_gb_tfidf_params      = gs_gb_tfidf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Transformer</th>\n",
       "      <th>Training Score</th>\n",
       "      <th>Testing Score</th>\n",
       "      <th>Best Parameters</th>\n",
       "      <th>Run Time (min)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.985955</td>\n",
       "      <td>0.753489</td>\n",
       "      <td>{'cvec__max_df': 0.9, 'cvec__max_features': 5000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 1)...</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>TFIDFVectorizer</td>\n",
       "      <td>0.985890</td>\n",
       "      <td>0.757683</td>\n",
       "      <td>{'rf__max_depth': None, 'rf__n_estimators': 100, 'rf__n_jobs': 6, 'tfidf__max_df': 0.9, 'tfidf__...</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.761107</td>\n",
       "      <td>0.747395</td>\n",
       "      <td>{'cvec__max_df': 0.9, 'cvec__max_features': 5000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 2)...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>TFIDFVectorizer</td>\n",
       "      <td>0.761041</td>\n",
       "      <td>0.743464</td>\n",
       "      <td>{'mnb__alpha': 0.01, 'tfidf__max_df': 0.9, 'tfidf__max_features': 5000, 'tfidf__min_df': 2, 'tfi...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.771504</td>\n",
       "      <td>0.761418</td>\n",
       "      <td>{'ada__base_estimator__max_depth': 2, 'ada__learning_rate': 0.9, 'ada__n_estimators': 150, 'cvec...</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>TFIDFVectorizer</td>\n",
       "      <td>0.771504</td>\n",
       "      <td>0.761680</td>\n",
       "      <td>{'ada__base_estimator__max_depth': 2, 'ada__learning_rate': 0.9, 'ada__n_estimators': 150, 'tfid...</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gradient Boost Classifier</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.800708</td>\n",
       "      <td>0.774196</td>\n",
       "      <td>{'cvec__max_df': 0.9, 'cvec__max_features': 3000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 1)...</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Gradient Boost Classifier</td>\n",
       "      <td>TFIDFVectorizer</td>\n",
       "      <td>0.799113</td>\n",
       "      <td>0.774523</td>\n",
       "      <td>{'gb__learning_rate': 0.9, 'gb__max_depth': 2, 'gb__n_estimators': 150, 'tfidf__max_df': 0.9, 't...</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Model      Transformer  Training Score  Testing Score  \\\n",
       "0              Random Forest  CountVectorizer        0.985955       0.753489   \n",
       "1              Random Forest  TFIDFVectorizer        0.985890       0.757683   \n",
       "2    Multinomial Naive Bayes  CountVectorizer        0.761107       0.747395   \n",
       "3    Multinomial Naive Bayes  TFIDFVectorizer        0.761041       0.743464   \n",
       "4       Ada Boost Classifier  CountVectorizer        0.771504       0.761418   \n",
       "5       Ada Boost Classifier  TFIDFVectorizer        0.771504       0.761680   \n",
       "6  Gradient Boost Classifier  CountVectorizer        0.800708       0.774196   \n",
       "7  Gradient Boost Classifier  TFIDFVectorizer        0.799113       0.774523   \n",
       "\n",
       "                                                                                       Best Parameters  \\\n",
       "0  {'cvec__max_df': 0.9, 'cvec__max_features': 5000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 1)...   \n",
       "1  {'rf__max_depth': None, 'rf__n_estimators': 100, 'rf__n_jobs': 6, 'tfidf__max_df': 0.9, 'tfidf__...   \n",
       "2  {'cvec__max_df': 0.9, 'cvec__max_features': 5000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 2)...   \n",
       "3  {'mnb__alpha': 0.01, 'tfidf__max_df': 0.9, 'tfidf__max_features': 5000, 'tfidf__min_df': 2, 'tfi...   \n",
       "4  {'ada__base_estimator__max_depth': 2, 'ada__learning_rate': 0.9, 'ada__n_estimators': 150, 'cvec...   \n",
       "5  {'ada__base_estimator__max_depth': 2, 'ada__learning_rate': 0.9, 'ada__n_estimators': 150, 'tfid...   \n",
       "6  {'cvec__max_df': 0.9, 'cvec__max_features': 3000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 1)...   \n",
       "7  {'gb__learning_rate': 0.9, 'gb__max_depth': 2, 'gb__n_estimators': 150, 'tfidf__max_df': 0.9, 't...   \n",
       "\n",
       "   Run Time (min)  \n",
       "0              48  \n",
       "1              45  \n",
       "2               3  \n",
       "3               3  \n",
       "4              81  \n",
       "5              81  \n",
       "6              68  \n",
       "7              68  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_data= {'Model'           : [\n",
    "                                    'Random Forest', \n",
    "                                    'Random Forest',\n",
    "                                    'Multinomial Naive Bayes',\n",
    "                                    'Multinomial Naive Bayes',\n",
    "                                    'Ada Boost Classifier',\n",
    "                                    'Ada Boost Classifier',\n",
    "                                    'Gradient Boost Classifier',\n",
    "                                    'Gradient Boost Classifier'\n",
    "                                    ], \n",
    "               'Transformer'     : [\n",
    "                                    'CountVectorizer',\n",
    "                                    'TFIDFVectorizer',\n",
    "                                    'CountVectorizer',\n",
    "                                    'TFIDFVectorizer',\n",
    "                                    'CountVectorizer',\n",
    "                                    'TFIDFVectorizer',\n",
    "                                    'CountVectorizer',\n",
    "                                    'TFIDFVectorizer'\n",
    "                                   ],\n",
    "               'Training Score'  : [\n",
    "                                    best_rf_cvec_train_score,\n",
    "                                    best_rf_tfidf_train_score,\n",
    "                                    best_mnb_cvec_train_score,\n",
    "                                    best_mnb_tfidf_train_score,\n",
    "                                    best_ada_cvec_train_score,\n",
    "                                    best_ada_tfidf_train_score,\n",
    "                                    best_gb_cvec_train_score,\n",
    "                                    best_gb_tfidf_train_score\n",
    "                                   ],\n",
    "               'Testing Score'   : [\n",
    "                                    best_rf_cvec_test_score,\n",
    "                                    best_rf_tfidf_test_score,\n",
    "                                    best_mnb_cvec_test_score,\n",
    "                                    best_mnb_tfidf_test_score,\n",
    "                                    best_ada_cvec_test_score,\n",
    "                                    best_ada_tfidf_test_score,\n",
    "                                    best_gb_cvec_test_score,\n",
    "                                    best_gb_tfidf_test_score\n",
    "                                   ],\n",
    "               'Best Parameters' : [\n",
    "                                    best_rf_cvec_params,\n",
    "                                    best_rf_tfidf_params,\n",
    "                                    best_mnb_cvec_params,\n",
    "                                    best_mnb_tfidf_params,\n",
    "                                    best_ada_cvec_params,\n",
    "                                    best_ada_tfidf_params,\n",
    "                                    best_gb_cvec_params,\n",
    "                                    best_gb_tfidf_params\n",
    "                                   ],\n",
    "               'Run Time (min)'        : [\n",
    "                                    int(round(rf_cvec_time   / 60)),\n",
    "                                    int(round(rf_tfidf_time  / 60)),\n",
    "                                    int(round(mnb_cvec_time  / 60)),\n",
    "                                    int(round(mnb_tfidf_time / 60)),\n",
    "                                    int(round(ada_cvec_time  / 60)),\n",
    "                                    int(round(ada_tfidf_time / 60)),\n",
    "                                    int(round(gb_cvec_time   / 60)),\n",
    "                                    int(round(gb_tfidf_time  / 60))\n",
    "                                   ]            \n",
    "              }\n",
    "\n",
    "results = pd.DataFrame(data=results_data)\n",
    "\n",
    "# Exporting results table to CSV to build visualizations\n",
    "results.to_csv('results.csv')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "whiskey    0.350108\n",
       "Scotch     0.327960\n",
       "bourbon    0.321932\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating baseline\n",
    "baseline = y_test.value_counts(normalize=True)\n",
    "baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights\n",
    "Each model performed very similarly no matter the vectorizer. Given the lack of structured data cleaning and only analyzing the submission title the lower scores seem reasonable given the general overlap that r/whiskey will have when compared to the more specialized r/bourbon and r/scotch.\n",
    "\n",
    "The baseline scores were about .33 for each subreddit which is to be expected by the even distribution in sample size. Each model performed well above the baseline with an apporximate accuracy of 75% for predicting submissions.\n",
    "\n",
    "##### Model Performance\n",
    "![proj3-model-performance.png](images/proj3-model-performance.png)\n",
    "\n",
    "While all models had similar accuracy scores the runtimes were very different. It's worth noting the very low processing required by the Multinomial Naive Bayes model means it should be considered in future modeling regimes.\n",
    "\n",
    "##### Run Time\n",
    "![proj3-time.png](images/proj3-time.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations and Next Steps\n",
    "\n",
    "#### Alter Stopwords Dictionary\n",
    "- The default 'english' stopwords dictionary contains the word 'still'. I considered this to be problematic for comparing various whiskeys and their manufacturers.\n",
    "\n",
    "#### Combine title and self text\n",
    "- Expand on data set by combining the 'title' and 'self text' data. Use the 'stopwordplaceholder' as a way to prevent nulls. Adding this string to the edited stopwords dictionary will remove it from analysis.\n",
    "\n",
    "#### Expand on Hyperparameters using AWS\n",
    "- Given the process limitations of my local machine I was limited to the hyperparameters I could gridsearch across all my models. Spinning up a virtual machine to expand processing power will allow for larger and more powerful grid searches.\n",
    "\n",
    "#### Fine Tune Pipelines\n",
    "- After Expanded GridSearching a better idea of what number of features that will be ideal will be known. Pulling out the most influential keywords for comparison may show additional insights.\n",
    "\n",
    "#### Compare just r/scotch and r/bourbon\n",
    "- Removing r/whiskey data from the comparison may show larger differences in model scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
